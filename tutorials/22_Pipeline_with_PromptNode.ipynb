{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OvkPji9O-qX"
      },
      "source": [
        "# Tutorial: Creating a Generative QA Pipeline with PromptNode\n",
        "\n",
        "- **Level**: Intermediate\n",
        "- **Time to complete**: 15 minutes\n",
        "- **Nodes Used**: `InMemoryDocumentStore`, `BM25Retriever`, `PromptNode`, `PromptTemplate`\n",
        "- **Goal**: After completing this tutorial, you'll have created a generative question answering search system that uses a large language model through PromptNode with the new PromptTemplate structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFqHcXYPO-qZ"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Learn how to build a generative question answering pipeline using the power of LLMs with PromptNode. In this tutorial, we'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents, but you can replace them with any text you want.\n",
        "\n",
        "This tutorial introduces you to the new PrompTemplate structure and explains how to use the new PrompTemplate to integrate PromptNode into a pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXjVlbPiO-qZ"
      },
      "source": [
        "## Preparing the Colab Environment\n",
        "\n",
        "- [Enable GPU Runtime in Colab](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n",
        "- [Set logging level to INFO](https://docs.haystack.deepset.ai/docs/log-level)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kww5B_vXO-qZ"
      },
      "source": [
        "## Installing Haystack\n",
        "\n",
        "To start, let's install the latest release of Haystack with `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQbU8GUfO-qZ"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab]\n",
        "pip install datasets>=2.6.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl_jYERtO-qa"
      },
      "source": [
        "### Enabling Telemetry\n",
        "\n",
        "Knowing you're using this tutorial helps us decide where to invest our efforts to build a better product but you can always opt out by commenting the following line. See [Telemetry](https://docs.haystack.deepset.ai/docs/telemetry) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A76B4S49O-qa"
      },
      "outputs": [],
      "source": [
        "from haystack.telemetry import tutorial_running\n",
        "\n",
        "tutorial_running(22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lvfew16O-qa"
      },
      "source": [
        "## Initializing the DocumentStore\n",
        "\n",
        "We'll start creating our question answering system by initializing a DocumentStore. A DocumentStore stores the Documents that the question answering system uses to find answers to your questions. In this tutorial, we're using the `InMemoryDocumentStore`.\n",
        "\n",
        "Let's initialize our DocumentStore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbVN-s5LO-qa"
      },
      "outputs": [],
      "source": [
        "from haystack.document_stores import InMemoryDocumentStore\n",
        "\n",
        "document_store = InMemoryDocumentStore(use_bm25=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8nuJdWO-qa"
      },
      "source": [
        "> `InMemoryDocumentStore` is the simplest DocumentStore to get started with. It requires no external dependencies and it's a good option for smaller projects and debugging. But it doesn't scale up so well to larger Document collections, so it's not a good choice for production systems. To learn more about the DocumentStore and the different types of external databases that we support, see [DocumentStore](https://docs.haystack.deepset.ai/docs/document_store)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvLVaFHTO-qb"
      },
      "source": [
        "The DocumentStore is now ready. Now it's time to fill it with some Documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HryYZP9ZO-qb"
      },
      "source": [
        "## Fetching and Writing Documents\n",
        "\n",
        "We'll use the Wikipedia pages of [Seven Wonders of the Ancient World](https://en.wikipedia.org/wiki/Wonders_of_the_World) as Documents. We preprocessed the data and uploaded to a Hugging Face Space: [Seven Wonders](https://huggingface.co/datasets/bilgeyucel/seven-wonders). Thus, we don't need to perform any additional cleaning or splitting.\n",
        "\n",
        "Let's fetch the data and write it to the DocumentStore:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INdC3WvLO-qb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"bilgeyucel/seven-wonders\", split=\"train\")\n",
        "\n",
        "document_store.write_documents(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_cj-5m-O-qb"
      },
      "source": [
        "## Initializing the Retriever\n",
        "\n",
        "Let's initialize a BM25Retriever and make it use the InMemoryDocumentStore we initialized earlier in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uo-6fjiO-qb"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import BM25Retriever\n",
        "\n",
        "retriever = BM25Retriever(document_store=document_store, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CEuQpB7O-qb"
      },
      "source": [
        "## Initializing the PromptNode\n",
        "\n",
        "Let's define a custom prompt for PromptTemplate to use with PromptNode. As parameters, this prompt will accept Documents that our Retriever fetched from our DocumentStore and `query` we pass at runtime. To join the content of the Documents, we'll use `join()` function. To learn about using functions in PromptTemplate, check out [PromptTemplate Structure](https://docs.haystack.deepset.ai/docs/prompt_node#prompttemplate-structure). Finally, we'll use [AnswerParser](https://docs.haystack.deepset.ai/reference/prompt-node-api#answerparser) to parse the output of the LLM into a Haystack Answer object.\n",
        "\n",
        "We'll initialize PromptNode with the PromptTemplate and `google/flan-t5-large` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6NFmpjEO-qb"
      },
      "outputs": [],
      "source": [
        "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
        "\n",
        "lfqa_prompt = PromptTemplate(\n",
        "    prompt=\"\"\"Synthesize a comprehensive answer from the following text for the given question.\n",
        "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
        "                             Your answer should be in your own words and be no longer than 50 words.\n",
        "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
        "    output_parser=AnswerParser(),\n",
        ")\n",
        "\n",
        "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=lfqa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIrajz1EO-qb"
      },
      "source": [
        ">To learn about how to use custom templates with PromptNode, check out [Customizing PromptNode for NLP Tasks](https://haystack.deepset.ai/tutorials/21_customizing_promptnode) tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfMCHStjO-qc"
      },
      "source": [
        "## Defining the Pipeline\n",
        "\n",
        "We'll use a custom pipeline with the Retriever, and PromptNode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZC9Dr-xO-qc"
      },
      "outputs": [],
      "source": [
        "from haystack.pipelines import Pipeline\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
        "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NqyLhx7O-qc"
      },
      "source": [
        "That's it! The pipeline's ready to generate answers to questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBAyF5tVO-qc"
      },
      "source": [
        "## Asking a Question\n",
        "\n",
        "We use the pipeline `run()` method to ask a question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vnt283M5O-qc"
      },
      "outputs": [],
      "source": [
        "output = pipe.run(query=\"How does Rhodes Statue look like?\")\n",
        "\n",
        "print(output[\"answers\"][0].answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWQN-aoGO-qc"
      },
      "source": [
        "Here are some other example queries to test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OHUQ5xxO-qc"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    \"Where is Gardens of Babylon?\",\n",
        "    \"Why did people build Great Pyramid of Giza?\",\n",
        "    \"How does Rhodes Statue look like?\",\n",
        "    \"Why did people visit the Temple of Artemis?\",\n",
        "    \"What is the importance of Colossus of Rhodes?\",\n",
        "    \"What happened to the Tomb of Mausolus?\",\n",
        "    \"How did Colossus of Rhodes collapse?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XueCK3y4O-qc"
      },
      "source": [
        "ðŸŽ‰ Congratulations! You've learned how to create a generative QA system for your documents with PromptNode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3jq25OFO-qc"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}